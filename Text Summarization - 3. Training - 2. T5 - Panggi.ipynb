{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcc2447-a05e-488c-a027-4f74147de12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate torch numpy pandas SentencePiece\n",
    "# !pip install transformers[torch]\n",
    "# !pip install accelerate==0.34.2\n",
    "# !pip show accelerate\n",
    "# !pip install absl-py rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5423f673-d1d7-4429-bb2b-47b1807c0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, GenerationConfig, T5Model\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, Trainer, EarlyStoppingCallback\n",
    "from transformers import ProgressCallback\n",
    "from transformers import TrainerCallback\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from evaluate import load\n",
    "import torch\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a143f91b-0069-4c6d-b31a-2e1c61608dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2879e65-afcf-4c6a-9ce5-a48cb10dacc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from parquet files using load_dataset\n",
    "dataset = load_dataset('parquet', data_files={'train': 'dataset/used/train.parquet', \n",
    "                                               'test': 'dataset/used/test.parquet', \n",
    "                                               'dev': 'dataset/used/dev.parquet',\n",
    "                                             'xtreme_test':'dataset/used/xtreme_test.parquet',\n",
    "                                             'xtreme_dev':'dataset/used/xtreme_dev.parquet'})\n",
    "\n",
    "# Drop unused columns\n",
    "columns_to_drop = ['id', 'url','clean_article','clean_summary','extractive_summary','ext_clean_article','ext_clean_summary','extractive_summary_sentences','ext_extractive_summary','prep_clean_article_no_sw']\n",
    "\n",
    "dataset['train'] = dataset['train'].remove_columns(columns_to_drop)\n",
    "dataset['test'] = dataset['test'].remove_columns(columns_to_drop)\n",
    "dataset['dev'] = dataset['dev'].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13200798-b6fb-497c-92dc-deaddcd64d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prep_clean_article', 'prep_clean_summary', 'prep_extractive_summary'],\n",
       "    num_rows: 193883\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d00b39-3e84-4502-bbe1-bd5f66f5494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = dataset[\"train\"].shuffle(seed=42).select(range(int(2500)))\n",
    "dev_sample = dataset[\"dev\"].shuffle(seed=42).select(range(int(700)))\n",
    "test_sample = dataset[\"test\"].shuffle(seed=42).select(range(int(700)))\n",
    "\n",
    "# train_sample = dataset[\"train\"].shuffle(seed=42).select(range(int(len(dataset[\"train\"]) * 0.005)))\n",
    "# dev_sample = dataset[\"dev\"].shuffle(seed=42).select(range(int(len(dataset[\"dev\"]) * 0.05)))\n",
    "# test_sample = dataset[\"test\"].shuffle(seed=42).select(range(int(len(dataset[\"test\"]) * 0.05)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff33112-87d0-421c-9f16-fa7aa2bd10be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 80\n",
      "Val size: 25\n",
      "Test size: 25\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_sample)}\")\n",
    "print(f\"Val size: {len(dev_sample)}\")\n",
    "print(f\"Test size: {len(test_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85478c1d-18e8-4d24-ba60-3d93ac256227",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': train_sample,\n",
    "    'val': dev_sample,\n",
    "    'test': test_sample\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5aa7656-3014-4fcc-9a48-bf6f812e2ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"panggi/t5-base-indonesian-summarization-cased\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"panggi/t5-base-indonesian-summarization-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f8f5a6-9cba-4a64-8feb-7f496ad8cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12438e93-1011-477e-88d5-4c8c07de94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"prep_clean_article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"prep_clean_summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9446d94-54a9-46a6-93f5-306c20ec3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21302626-3b03-4118-a549-774da449c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'whatsapp.inspektorat.pekalongankab.go.id'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from_number = '6285741880658@c.us'\n",
    "\n",
    "hasil = f'Memulai Pelatihan Model \\nTrain size: {len(train_sample)} \\nVal size: {len(dev_sample)} \\nTest size: {len(test_sample)}'\n",
    "\n",
    "response = requests.post(\n",
    "        'https://whatsapp.inspektorat.pekalongankab.go.id/api/sendText/', #rahasia ya ;D\n",
    "        headers={\n",
    "            'Content-Type': 'application/json; charset=utf-8',\n",
    "            'Accept': 'application/json',\n",
    "            'X-Api-Key': 'KMZWAY87AA' # rahasia\n",
    "        },\n",
    "        json={  # Menggunakan parameter `json` untuk mengirim raw JSON\n",
    "            'chatId': from_number,\n",
    "            'text': hasil,\n",
    "            'session': 'NoamChomsky'\n",
    "        },\n",
    "        verify=False  # Ini sesuai dengan withoutVerifying() di PHP\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a922615-2bf9-4d9f-8cf2-85c361d1779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Catat training loss\n",
    "        training_loss = state.log_history[-1]['loss'] if state.log_history else None\n",
    "        state.log_history.append({\n",
    "            'epoch': state.epoch,\n",
    "            'training_loss': training_loss\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "583bd3f3-2fcc-4ff9-b798-d09f7ae845e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.get_encoder().parameters():\n",
    "    param.requires_grad = False  # Membekukan semua parameter encoder\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",   \n",
    "    save_strategy=\"epoch\",         \n",
    "    logging_dir='./logs',          \n",
    "    logging_steps=10,              \n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,  # Ubah batch size dari 4 menjadi 2\n",
    "    per_device_eval_batch_size=2,   # Ubah eval batch size dari 4 menjadi 2\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.03,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge2\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "421ac03b-4251-4e88-a3a7-94e81d3b0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load('rouge')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]  # Memperbaiki typo di sini, sebelumnya 'preds' seharusnya 'predictions'\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Directly decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects newline-separated text\n",
    "    decoded_preds = [\"\\n\".join(decoded_pred.split()) for decoded_pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(decoded_label.split()) for decoded_label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96565150-9a35-4265-a7af-686e8b185841",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)  # Mengatur jumlah threads sesuai dengan jumlah core CPU\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,  # Pastikan ini sudah teratur\n",
    "    callbacks=[\n",
    "        CustomCallback(),  \n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        ProgressCallback()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed578b48-05d0-4f8f-8eec-a4e8bfcc3f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaab7f269c9f430ba3c042b110491f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 04:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.930900</td>\n",
       "      <td>3.263117</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.701000</td>\n",
       "      <td>3.203314</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>0.108100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8876, 'grad_norm': 3.7228009700775146, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 2.9309, 'grad_norm': 3.758315086364746, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2631173133850098, 'eval_rouge1': 0.0795, 'eval_rouge2': 0.0304, 'eval_rougeL': 0.0695, 'eval_rougeLsum': 0.0804, 'eval_runtime': 47.8264, 'eval_samples_per_second': 0.523, 'eval_steps_per_second': 0.272, 'epoch': 1.0}\n",
      "{'loss': 2.4312, 'grad_norm': 3.7096760272979736, 'learning_rate': 1.25e-05, 'epoch': 1.5}\n",
      "{'loss': 2.701, 'grad_norm': 3.902181625366211, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2033135890960693, 'eval_rouge1': 0.1101, 'eval_rouge2': 0.0385, 'eval_rougeL': 0.0926, 'eval_rougeLsum': 0.1081, 'eval_runtime': 47.9872, 'eval_samples_per_second': 0.521, 'eval_steps_per_second': 0.271, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 297.2023, 'train_samples_per_second': 0.538, 'train_steps_per_second': 0.135, 'train_loss': 2.737675428390503, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=2.737675428390503, metrics={'train_runtime': 297.2023, 'train_samples_per_second': 0.538, 'train_steps_per_second': 0.135, 'total_flos': 47834118051840.0, 'train_loss': 2.737675428390503, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "103112c0-8f49-41a4-b56c-beb82b5c966e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/t5-panggi/tokenizer_config.json',\n",
       " 'model/t5-panggi/special_tokens_map.json',\n",
       " 'model/t5-panggi/spiece.model',\n",
       " 'model/t5-panggi/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menyimpan model\n",
    "model.save_pretrained(\"model/t5-panggi\")\n",
    "\n",
    "# Menyimpan tokenizer\n",
    "tokenizer.save_pretrained(\"model/t5-panggi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bda6ce99-9618-4dae-a085-68f61c702bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendapatkan log history dari trainer\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Membuat DataFrame dari log_history\n",
    "df_log_history = pd.DataFrame(log_history)\n",
    "\n",
    "# Simpan ke file CSV\n",
    "df_log_history.to_csv('log_history.csv', index=False)\n",
    "\n",
    "train_runtime_minutes = ''\n",
    "\n",
    "# Mencari log yang berisi train_runtime\n",
    "for log in log_history:\n",
    "    if 'train_runtime' in log:\n",
    "        train_runtime = log['train_runtime']\n",
    "        train_runtime_minutes = train_runtime / 60  # Konversi ke menit\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "280519b9-df29-4282-b2cc-93c56ecbcd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'whatsapp.inspektorat.pekalongankab.go.id'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from_number = '6285741880658@c.us'\n",
    "\n",
    "hasil = f'Selesai Pelatihan Model : {train_runtime_minutes:.2f} menit'\n",
    "\n",
    "response = requests.post(\n",
    "        'https://whatsapp.inspektorat.pekalongankab.go.id/api/sendText/', #rahasia ya ;D\n",
    "        headers={\n",
    "            'Content-Type': 'application/json; charset=utf-8',\n",
    "            'Accept': 'application/json',\n",
    "            'X-Api-Key': 'KMZWAY87AA' # rahasia\n",
    "        },\n",
    "        json={  # Menggunakan parameter `json` untuk mengirim raw JSON\n",
    "            'chatId': from_number,\n",
    "            'text': hasil,\n",
    "            'session': 'NoamChomsky'\n",
    "        },\n",
    "        verify=False  # Ini sesuai dengan withoutVerifying() di PHP\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cbcc945-eb54-409f-9785-f2792e39609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train runtime: 4.95 menit\n"
     ]
    }
   ],
   "source": [
    "# Mendapatkan log history dari trainer\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Mencari log yang berisi train_runtime\n",
    "for log in log_history:\n",
    "    if 'train_runtime' in log:\n",
    "        train_runtime = log['train_runtime']\n",
    "        train_runtime_minutes = train_runtime / 60  # Konversi ke menit\n",
    "        print(f\"Train runtime: {train_runtime_minutes:.2f} menit\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6f3467c-575f-4702-ab50-06b646937020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Memuat tokenizer\n",
    "# tokenizerx = T5Tokenizer.from_pretrained(\"model\")\n",
    "\n",
    "# # Memuat model\n",
    "# modelx = T5ForConditionalGeneration.from_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "959528bc-8915-4b52-a625-16163ab4dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# textx = \"\"\"\n",
    "#     Liputan6.com, Bandung: Ratusan warga Kecamatan Andir, Kota Madya Bandung, Jawa Barat, antusias mengikuti sosialisasi teknik pencoblosan Pemilihan Umum 2004 yang digelar Komisi Pemilihan Umum Daerah Bandung, Senin(9/2). Walau secara keseluruhan tak ada kesulitan, warga tetap mengaku bingung. Soalnya ukuran kertas suara lebih besar dibanding ukuran bilik suara. Dalam sosialisasi teknis pencoblosan, KPU Bandung memang langsung memperagakan dengan menggunakan bilik suara asli serta kertas suara sesuai ukuran yang sebenarnya. Hal ini dimaksudkan agar dalam pelaksanaan pemilu nanti para calon pemilih tak mengalami kesulitan lagi. Seperti diketahui, kertas suara berukuran 48 X 84 sentimeter. Sementara bilik suara hanya berukuran 50 X 50 sentimeter dengan tinggi 60 sentimeter [baca: Pemilu Sekarang Memang Berbeda].(ICH/Patria Hidayat dan Taufik Hidayat).\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f27730e-bd5a-4950-922c-b4b1e148f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenisasi input\n",
    "# inputsx = tokenizerx.encode(textx, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# # Menghasilkan ringkasan\n",
    "# summary_idsx = modelx.generate(inputsx, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# # Mengubah token kembali ke teks\n",
    "# summaryx = tokenizerx.decode(summary_idsx[0], skip_special_tokens=True)\n",
    "\n",
    "# # Menampilkan ringkasan\n",
    "# print(\"Ringkasan:\", summaryx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d79782e-4287-4513-ba93-5fdfc6d4aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenisasi input\n",
    "# inputsx = tokenizerx.encode(textx, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# # Menghasilkan ringkasan\n",
    "summary_idsx = modelx.generate(inputsx,\n",
    "            max_length=100, \n",
    "            num_beams=2,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            use_cache=True)\n",
    "\n",
    "# # Mengubah token kembali ke teks\n",
    "# summaryx = tokenizerx.decode(summary_idsx[0], skip_special_tokens=True)\n",
    "\n",
    "# # Menampilkan ringkasan\n",
    "# print(\"Ringkasan:\", summaryx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "613f4e14-cfd6-444a-8487-3843c9113028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
    "#   warnings.warn(\n",
    "# {'eval_loss': 2.4181246757507324, 'eval_rouge1': 0.3015, 'eval_rouge2': 0.1477, 'eval_rougeL': 0.2614, 'eval_rougeLsum': 0.3011, 'eval_runtime': 996.3124, 'eval_samples_per_second': 0.55, 'eval_steps_per_second': 0.138, 'epoch': 1.0}\n",
    "# /usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
    "#   warnings.warn(\n",
    "# {'eval_loss': 2.427324056625366, 'eval_rouge1': 0.3012, 'eval_rouge2': 0.1466, 'eval_rougeL': 0.2616, 'eval_rougeLsum': 0.3011, 'eval_runtime': 1010.0476, 'eval_samples_per_second': 0.543, 'eval_steps_per_second': 0.136, 'epoch': 2.0}\n",
    "# /usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
    "#   warnings.warn(\n",
    "# {'eval_loss': 2.436957597732544, 'eval_rouge1': 0.2982, 'eval_rouge2': 0.1444, 'eval_rougeL': 0.259, 'eval_rougeLsum': 0.2982, 'eval_runtime': 1011.9433, 'eval_samples_per_second': 0.542, 'eval_steps_per_second': 0.135, 'epoch': 2.99}\n",
    "# There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
    "# {'train_runtime': 11335.3937, 'train_samples_per_second': 0.256, 'train_steps_per_second': 0.032, 'train_loss': 1.7137528516701102, 'epoch': 2.99}\n",
    "# TrainOutput(global_step=363, training_loss=1.7137528516701102, metrics={'train_runtime': 11335.3937, 'train_samples_per_second': 0.256, 'train_steps_per_second': 0.032, 'total_flos': 1285241309153280.0, 'train_loss': 1.7137528516701102, 'epoch': 2.9876543209876543})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
