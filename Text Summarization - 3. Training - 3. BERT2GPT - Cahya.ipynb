{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcc2447-a05e-488c-a027-4f74147de12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate torch numpy pandas SentencePiece transformers[torch] accelerate==0.34.2 absl-py rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5423f673-d1d7-4429-bb2b-47b1807c0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, Trainer, EarlyStoppingCallback\n",
    "from transformers import ProgressCallback\n",
    "from transformers import TrainerCallback\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from evaluate import load\n",
    "import torch\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a143f91b-0069-4c6d-b31a-2e1c61608dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2879e65-afcf-4c6a-9ce5-a48cb10dacc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from parquet files using load_dataset\n",
    "dataset = load_dataset('parquet', data_files={'train': 'dataset/used/train.parquet', \n",
    "                                               'test': 'dataset/used/test.parquet', \n",
    "                                               'dev': 'dataset/used/dev.parquet'})\n",
    "\n",
    "# Drop unused columns\n",
    "columns_to_drop = ['id', 'url','clean_article','clean_summary','extractive_summary','ext_clean_article','ext_clean_summary','extractive_summary_sentences','ext_extractive_summary','prep_clean_article_no_sw']\n",
    "\n",
    "dataset['train'] = dataset['train'].remove_columns(columns_to_drop)\n",
    "dataset['test'] = dataset['test'].remove_columns(columns_to_drop)\n",
    "dataset['dev'] = dataset['dev'].remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13200798-b6fb-497c-92dc-deaddcd64d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prep_clean_article', 'prep_clean_summary', 'prep_extractive_summary'],\n",
       "    num_rows: 193883\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d00b39-3e84-4502-bbe1-bd5f66f5494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = dataset[\"train\"].shuffle(seed=42).select(range(int(3000)))\n",
    "dev_sample = dataset[\"dev\"].shuffle(seed=42).select(range(int(1000)))\n",
    "test_sample = dataset[\"test\"].shuffle(seed=42).select(range(int(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff33112-87d0-421c-9f16-fa7aa2bd10be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 50\n",
      "Val size: 10\n",
      "Test size: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_sample)}\")\n",
    "print(f\"Val size: {len(dev_sample)}\")\n",
    "print(f\"Test size: {len(test_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85478c1d-18e8-4d24-ba60-3d93ac256227",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': train_sample,\n",
    "    'val': dev_sample,\n",
    "    'test': test_sample\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5aa7656-3014-4fcc-9a48-bf6f812e2ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"cahya/bert2gpt-indonesian-summarization\")\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "model = EncoderDecoderModel.from_pretrained(\"cahya/bert2gpt-indonesian-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f8f5a6-9cba-4a64-8feb-7f496ad8cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12438e93-1011-477e-88d5-4c8c07de94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"prep_clean_article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"prep_clean_summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9446d94-54a9-46a6-93f5-306c20ec3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21302626-3b03-4118-a549-774da449c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'whatsapp.inspektorat.pekalongankab.go.id'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from_number = '6285741880658@c.us'\n",
    "\n",
    "hasil = f'Memulai Pelatihan Model BERT2GPT \\nTrain size: {len(train_sample)} \\nVal size: {len(dev_sample)} \\nTest size: {len(test_sample)}'\n",
    "\n",
    "response = requests.post(\n",
    "        'https://whatsapp.inspektorat.pekalongankab.go.id/api/sendText/', #rahasia ya ;D\n",
    "        headers={\n",
    "            'Content-Type': 'application/json; charset=utf-8',\n",
    "            'Accept': 'application/json',\n",
    "            'X-Api-Key': 'KMZWAY87AA' # rahasia\n",
    "        },\n",
    "        json={  # Menggunakan parameter `json` untuk mengirim raw JSON\n",
    "            'chatId': from_number,\n",
    "            'text': hasil,\n",
    "            'session': 'NoamChomsky'\n",
    "        },\n",
    "        verify=False  # Ini sesuai dengan withoutVerifying() di PHP\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "583bd3f3-2fcc-4ff9-b798-d09f7ae845e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.get_encoder().parameters():\n",
    "    param.requires_grad = False  # Membekukan semua parameter encoder\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",   \n",
    "    save_strategy=\"epoch\",         \n",
    "    logging_dir='./logs',          \n",
    "    logging_steps=10,              \n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.05,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge2\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "421ac03b-4251-4e88-a3a7-94e81d3b0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load('rouge')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Directly decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=False)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "\n",
    "    # Rouge expects newline-separated text\n",
    "    decoded_preds = [\"\\n\".join(decoded_pred.split()) for decoded_pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(decoded_label.split()) for decoded_label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96565150-9a35-4265-a7af-686e8b185841",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(24)  # Mengatur jumlah threads sesuai dengan jumlah core CPU\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,  # Pastikan ini sudah teratur\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        ProgressCallback()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed578b48-05d0-4f8f-8eec-a4e8bfcc3f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718c8dba2c7b4590ac42c6d07e49933f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 15:18, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.392625</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.426100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.690100</td>\n",
       "      <td>3.156455</td>\n",
       "      <td>0.502600</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.392624616622925, 'eval_rouge1': 0.4282, 'eval_rouge2': 0.2349, 'eval_rougeL': 0.3668, 'eval_rougeLsum': 0.4261, 'eval_runtime': 283.5415, 'eval_samples_per_second': 0.035, 'eval_steps_per_second': 0.011, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 40, 'min_length': 20, 'early_stopping': True, 'num_beams': 10, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6901, 'grad_norm': 5.679691314697266, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1692073345184326, 'eval_rouge1': 0.4715, 'eval_rouge2': 0.3029, 'eval_rougeL': 0.4291, 'eval_rougeLsum': 0.4718, 'eval_runtime': 240.3992, 'eval_samples_per_second': 0.042, 'eval_steps_per_second': 0.012, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1564552783966064, 'eval_rouge1': 0.5026, 'eval_rouge2': 0.3342, 'eval_rougeL': 0.4569, 'eval_rougeLsum': 0.5011, 'eval_runtime': 238.4112, 'eval_samples_per_second': 0.042, 'eval_steps_per_second': 0.013, 'epoch': 2.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['decoder.lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 929.2966, 'train_samples_per_second': 0.161, 'train_steps_per_second': 0.019, 'train_loss': 2.323039107852512, 'epoch': 2.77}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18, training_loss=2.323039107852512, metrics={'train_runtime': 929.2966, 'train_samples_per_second': 0.161, 'train_steps_per_second': 0.019, 'total_flos': 47322787276800.0, 'train_loss': 2.323039107852512, 'epoch': 2.769230769230769})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "103112c0-8f49-41a4-b56c-beb82b5c966e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/bert2gpt-cahya/tokenizer_config.json',\n",
       " 'model/bert2gpt-cahya/special_tokens_map.json',\n",
       " 'model/bert2gpt-cahya/vocab.txt',\n",
       " 'model/bert2gpt-cahya/added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menyimpan model\n",
    "model.save_pretrained(\"model/bert2gpt-cahya\")\n",
    "\n",
    "# Menyimpan tokenizer\n",
    "tokenizer.save_pretrained(\"model/bert2gpt-cahya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bda6ce99-9618-4dae-a085-68f61c702bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendapatkan log history dari trainer\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Membuat DataFrame dari log_history\n",
    "df_log_history = pd.DataFrame(log_history)\n",
    "\n",
    "# Simpan ke file CSV\n",
    "df_log_history.to_csv('log_history.csv', index=False)\n",
    "\n",
    "train_runtime_minutes = ''\n",
    "\n",
    "# Mencari log yang berisi train_runtime\n",
    "for log in log_history:\n",
    "    if 'train_runtime' in log:\n",
    "        train_runtime = log['train_runtime']\n",
    "        train_runtime_minutes = train_runtime / 60  # Konversi ke menit\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "280519b9-df29-4282-b2cc-93c56ecbcd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'whatsapp.inspektorat.pekalongankab.go.id'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from_number = '6285741880658@c.us'\n",
    "response = requests.post(\n",
    "        'https://whatsapp.inspektorat.pekalongankab.go.id/api/sendText/', #rahasia ya ;D\n",
    "        headers={\n",
    "            'Content-Type': 'application/json; charset=utf-8',\n",
    "            'Accept': 'application/json',\n",
    "            'X-Api-Key': 'KMZWAY87AA' # rahasia\n",
    "        },\n",
    "        json={  # Menggunakan parameter `json` untuk mengirim raw JSON\n",
    "            'chatId': from_number,\n",
    "            'text': f\"Selesai Pelatihan Model BERT2GPT : {train_runtime_minutes:.2f} menit\",\n",
    "            'session': 'NoamChomsky'\n",
    "        },\n",
    "        verify=False  # Ini sesuai dengan withoutVerifying() di PHP\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
